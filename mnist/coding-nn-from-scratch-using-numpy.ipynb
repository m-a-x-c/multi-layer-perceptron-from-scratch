{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da92c1a5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-10T05:37:58.003598Z",
     "iopub.status.busy": "2022-02-10T05:37:58.002355Z",
     "iopub.status.idle": "2022-02-10T05:37:58.015852Z",
     "shell.execute_reply": "2022-02-10T05:37:58.016413Z",
     "shell.execute_reply.started": "2022-02-09T19:24:59.872793Z"
    },
    "papermill": {
     "duration": 0.028246,
     "end_time": "2022-02-10T05:37:58.016750",
     "exception": false,
     "start_time": "2022-02-10T05:37:57.988504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777aaafc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T05:37:58.038941Z",
     "iopub.status.busy": "2022-02-10T05:37:58.037872Z",
     "iopub.status.idle": "2022-02-10T05:37:58.088371Z",
     "shell.execute_reply": "2022-02-10T05:37:58.087717Z",
     "shell.execute_reply.started": "2022-02-09T19:24:59.891176Z"
    },
    "papermill": {
     "duration": 0.0624,
     "end_time": "2022-02-10T05:37:58.088531",
     "exception": false,
     "start_time": "2022-02-10T05:37:58.026131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class HyperParameters:\n",
    "    def __init__(self, learning_rate=0.01, epochs=10, mini_batch_size=None, beta=.9, layers=None, beta1=.9, beta2=.998, lambd=0):\n",
    "        if layers is None:\n",
    "            layers = [10, 20, 10]\n",
    "        self.layers = layers\n",
    "        self.no_l = len(layers)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.beta = beta\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.lambd = lambd\n",
    "\n",
    "'''        \n",
    "Class that is responsbile for initialising weights and biases. \n",
    "'''\n",
    "class WeightAndBias: \n",
    "    def __init__(self, number_features, layers, initialisation_type=\"random\"):\n",
    "        \n",
    "        self.initialisation_type = initialisation_type\n",
    "        self.layers = [number_features] + layers\n",
    "        self.weights = [pd.DataFrame()] + [np.random.randn(self.layers[i+1], self.layers[i]) * 0.01 for i in range(len(self.layers)-1)]\n",
    "        self.biases = [pd.DataFrame()] + [np.zeros([self.layers[i+1], 1]) for i in range(len(self.layers)-1)]\n",
    "                \n",
    "    ''' \n",
    "    method to update learning parameters    \n",
    "    '''\n",
    "    def update_learning_parameters(self, no_l, hp_obj, dW, db, m_training) :\n",
    "        for l in range(1, no_l+1):\n",
    "            self.biases[l] =  self.biases[l] - hp_obj.learning_rate * db[l]\n",
    "            self.weights[l] = (1 - (hp_obj.lambd *  hp_obj.learning_rate)/m_training) * self.weights[l] - hp_obj.learning_rate * dW[l]\n",
    "\n",
    "'''            \n",
    "ActivationFunctions that takes layers and list of activation functions to be used for each of the layers.\n",
    "'''\n",
    "class ActivationFunctions:\n",
    "    def __init__(self, layers, activation_functions=None) :\n",
    "        if activation_functions is None: \n",
    "            activation_functions= ['tanh'] * (len(layers) - 1) + ['softmax']\n",
    "            \n",
    "        self.activation_functions = [None] + [eval(f'ActivationFunctions.{activation_function}') \n",
    "                                     for activation_function in activation_functions]\n",
    "        \n",
    "        self.derivative_functions = [None] + [eval(f'ActivationFunctions.{activation_function}_derivative') \n",
    "                                     for activation_function in activation_functions]\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(z) :\n",
    "        return 1 / (1 + np.exp( -z ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z) : \n",
    "        return np.where(z>0, z, 0.0001 * z )\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z) :\n",
    "        # return np.tanh(z\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return (np.exp(z) - np.exp(-z))/ (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0) \n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_derivative(y, a) :\n",
    "        return a - y\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(y, a) :\n",
    "        return a - y\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z) :\n",
    "        return (1 - np.tanh(z) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z) :\n",
    "        return (z > 0) * 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_loss(a, y, m, hp, lp) :\n",
    "            return (-1/m * np.sum(np.multiply(y, np.log(a))), \n",
    "                    -1/m * np.sum(np.multiply(y, np.log(a))) + hp.lambd/(2 *m ) * sum(np.sum(np.square(lp.weights[i]))\n",
    "                                                                                    for i in range(1, hp.no_l+1)))\n",
    "    \n",
    "'''\n",
    "NeuralNetwork class where the magic happens, Forward prop and Backprop happens.\n",
    "'''\n",
    "class NeuralNetwork: \n",
    "    def __init__(self, X_train, y_train, HyperParameters, activation_functions=None) :\n",
    "        \n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.n, self.m = X_train.shape\n",
    "        \n",
    "        print(f\"number of training examples: {self.m}\\nnumber of features: {self.n}\"\n",
    "              f\"\\nshape of y_train {self.y_train.shape}\")\n",
    "\n",
    "        #hp --> hyperparameters\n",
    "        self.hp = HyperParameters \n",
    "        self.layers = self.hp.layers\n",
    "        self.no_l = self.hp.no_l\n",
    "        \n",
    "        self.act_function_obj = ActivationFunctions(self.layers, activation_functions=activation_functions)\n",
    " \n",
    "        #lp --> learning parameters -> weights and biases\n",
    "        self.lp = WeightAndBias(self.n, self.layers) \n",
    "    \n",
    "        if self.hp.batch_size is None:\n",
    "            self.hp.batch_size = self.m\n",
    "            \n",
    "\n",
    "    def forward_propagation(self, X_batch) :\n",
    "        self.Z, self.A = [0] + [None] * self.no_l, [X_batch ] + [None] * self.no_l\n",
    "        activation_functions = self.act_function_obj.activation_functions\n",
    "\n",
    "        for l in range(1, self.no_l + 1):\n",
    "            self.Z[l] = np.dot(self.lp.weights[l], self.A[l-1]) + self.lp.biases[l]\n",
    "            self.A[l] = activation_functions[l](self.Z[l])      \n",
    "\n",
    "    def back_propagation(self, y_batch) :\n",
    "\n",
    "        derivative_functions = self.act_function_obj.derivative_functions\n",
    "        batch_size = y_batch.shape[1]\n",
    "        \n",
    "        self.dZ =[None] +  [None] * self.no_l\n",
    "        self.dW =[None] +  [None] * self.no_l\n",
    "        self.db =[None] +  [None] * self.no_l\n",
    "\n",
    "        self.dZ[self.no_l] = derivative_functions[self.no_l](y_batch, self.A[self.no_l])\n",
    "        self.dW[self.no_l] = 1/batch_size * np.dot(self.dZ[self.no_l] , self.A[self.no_l - 1].T)\n",
    "        self.db[self.no_l] = 1/batch_size * np.sum(self.dZ[self.no_l], axis=1, keepdims=True)\n",
    "\n",
    "        assert self.dZ[self.no_l].shape == self.Z[self.no_l].shape\n",
    "        assert self.db[self.no_l].shape == self.lp.biases[self.no_l].shape        \n",
    "        assert self.dW[self.no_l].shape == self.lp.weights[self.no_l].shape\n",
    "\n",
    "        for l in range(self.no_l - 1, 0, -1) : \n",
    "\n",
    "            self.dZ[l] = np.dot(self.lp.weights[l+1].T, self.dZ[l+1] )* derivative_functions[l](self.Z[l])\n",
    "            self.dW[l] = 1/batch_size * np.dot(self.dZ[l], self.A[l-1].T)\n",
    "            self.db[l] = 1/batch_size * np.sum(self.dZ[l], axis=1, keepdims=True)\n",
    "\n",
    "            assert self.dZ[l].shape == self.Z[l].shape\n",
    "            assert self.dW[l].shape == self.lp.weights[l].shape\n",
    "            assert self.db[l].shape == self.lp.biases[l].shape  \n",
    "            \n",
    "            \n",
    "    def train_nn(self, verbose=False, per_epoch_log=100) :\n",
    "        for epoch in range(self.hp.epochs): \n",
    "            for batch_s in range(0, self.m, self.hp.batch_size) :\n",
    "                \n",
    "                batch_e = min(batch_s + self.hp.batch_size, self.m)\n",
    "                \n",
    "                X_batch = self.X_train[:, batch_s: batch_e]\n",
    "                y_batch = self.y_train[:, batch_s: batch_e]\n",
    "                m_batch_size = batch_e - batch_s\n",
    "\n",
    "                self.forward_propagation(X_batch)\n",
    "                self.back_propagation(y_batch)\n",
    "                self.lp.update_learning_parameters(self.no_l, self.hp,  self.dW, self.db, m_batch_size)\n",
    "\n",
    "            if verbose and epoch % per_epoch_log == 0: \n",
    "                print(f\"epochs {epoch} loss: \",ActivationFunctions.calculate_loss(self.A[self.no_l], y_batch, m_batch_size, self.hp,  \n",
    "                                                                                  self.lp))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        self.forward_propagation(X_test)\n",
    "        preds=  self.A[self.no_l].T\n",
    "        return (preds == preds.max(axis=1)[:,None]).astype(int)\n",
    "\n",
    "def one_hot_encoding_y(train_data) :\n",
    "    a = train_data.label\n",
    "    b = np.zeros((a.size, 10))\n",
    "    b[np.arange(a.size),a] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c939f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T05:37:58.111408Z",
     "iopub.status.busy": "2022-02-10T05:37:58.110405Z",
     "iopub.status.idle": "2022-02-10T05:38:02.323590Z",
     "shell.execute_reply": "2022-02-10T05:38:02.323002Z",
     "shell.execute_reply.started": "2022-02-09T19:24:59.929491Z"
    },
    "papermill": {
     "duration": 4.226903,
     "end_time": "2022-02-10T05:38:02.323768",
     "exception": false,
     "start_time": "2022-02-10T05:37:58.096865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2004c5b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T05:38:02.344751Z",
     "iopub.status.busy": "2022-02-10T05:38:02.344070Z",
     "iopub.status.idle": "2022-02-10T05:49:51.134601Z",
     "shell.execute_reply": "2022-02-10T05:49:51.135777Z",
     "shell.execute_reply.started": "2022-02-09T19:25:02.737917Z"
    },
    "papermill": {
     "duration": 708.803983,
     "end_time": "2022-02-10T05:49:51.136402",
     "exception": false,
     "start_time": "2022-02-10T05:38:02.332419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples: 42000\n",
      "number of features: 784\n",
      "shape of y_train (10, 42000)\n",
      "epochs 0 loss:  (1.183773680744079, 1.185490161822964)\n",
      "epochs 10 loss:  (0.21547382936325504, 0.21985174022926463)\n",
      "epochs 20 loss:  (0.1408202103371239, 0.14668417831298255)\n",
      "epochs 30 loss:  (0.0982197614891936, 0.10538648778049989)\n",
      "epochs 40 loss:  (0.07105613454820482, 0.07938684893561114)\n",
      "epochs 50 loss:  (0.05317434554385173, 0.06256163777776504)\n",
      "epochs 60 loss:  (0.0413090525415835, 0.05166543438159734)\n",
      "epochs 70 loss:  (0.032918313007260794, 0.04417391137296511)\n",
      "epochs 80 loss:  (0.026738097130996993, 0.03883247257436905)\n",
      "epochs 90 loss:  (0.022206989611361187, 0.03508689923889535)\n",
      "epochs 100 loss:  (0.018844447584489384, 0.03246152583234133)\n",
      "epochs 110 loss:  (0.016319240052911053, 0.030629577854840946)\n",
      "epochs 120 loss:  (0.014385596010992657, 0.02934818896295907)\n",
      "epochs 130 loss:  (0.012804411360465742, 0.028380596765228937)\n",
      "epochs 140 loss:  (0.011545497639632247, 0.027698876140105536)\n",
      "epochs 150 loss:  (0.010466284215140119, 0.027162467198381744)\n",
      "epochs 160 loss:  (0.00957740456726605, 0.02678394523814759)\n",
      "epochs 170 loss:  (0.00882141847097254, 0.026507883995426723)\n",
      "epochs 180 loss:  (0.008170704975780098, 0.026308335932268216)\n",
      "epochs 190 loss:  (0.007610490285874524, 0.02617245108741075)\n",
      "epochs 200 loss:  (0.0071088685137143046, 0.026069935921988953)\n",
      "epochs 210 loss:  (0.006676997370349509, 0.026013527352630936)\n",
      "epochs 220 loss:  (0.006288475146228151, 0.025978252771976032)\n",
      "epochs 230 loss:  (0.005942223243963727, 0.025964442171387603)\n",
      "epochs 240 loss:  (0.0056284228679559616, 0.025963502841116797)\n",
      "epochs 250 loss:  (0.005352681340501861, 0.025982221017566337)\n",
      "epochs 260 loss:  (0.005097050897395739, 0.026003692418922215)\n",
      "epochs 270 loss:  (0.004866881023783488, 0.02603435180851349)\n",
      "epochs 280 loss:  (0.004653487013983548, 0.026066417067308993)\n",
      "epochs 290 loss:  (0.004467112549744913, 0.026111117981423237)\n",
      "epochs 300 loss:  (0.004290422992009157, 0.026152012507631887)\n",
      "epochs 310 loss:  (0.004131256999170897, 0.026197764835069776)\n",
      "epochs 320 loss:  (0.003987720233622139, 0.026247296503402972)\n",
      "epochs 330 loss:  (0.00385716177145412, 0.026298723749694834)\n",
      "epochs 340 loss:  (0.003734558178685949, 0.026347687409512103)\n",
      "epochs 350 loss:  (0.0036208719931011646, 0.02639578986305857)\n",
      "epochs 360 loss:  (0.0035186163233601134, 0.02644612340991845)\n",
      "epochs 370 loss:  (0.0034199753597561012, 0.02649143807619701)\n",
      "epochs 380 loss:  (0.0033307152822993017, 0.026537990231331313)\n",
      "epochs 390 loss:  (0.0032482177847661495, 0.026583647532325512)\n",
      "epochs 400 loss:  (0.0031701498406298658, 0.026626538015099153)\n",
      "epochs 410 loss:  (0.0030989697765825587, 0.02666953815498399)\n",
      "epochs 420 loss:  (0.00303210207204565, 0.026710468148597292)\n",
      "epochs 430 loss:  (0.0029690200847648806, 0.026749162847356414)\n",
      "epochs 440 loss:  (0.002911072166141568, 0.026787290311527413)\n",
      "epochs 450 loss:  (0.002857290285852222, 0.0268242085754504)\n",
      "epochs 460 loss:  (0.002805066137508698, 0.02685757329635818)\n",
      "epochs 470 loss:  (0.002757130081367799, 0.026890392848167067)\n",
      "epochs 480 loss:  (0.002714515828982312, 0.02692396302513326)\n",
      "epochs 490 loss:  (0.0026708304512403904, 0.026952123348097143)\n"
     ]
    }
   ],
   "source": [
    "m = train_data.shape[0]\n",
    "X = train_data.drop('label', axis=1).iloc[0:m].to_numpy() / 255\n",
    "y  = one_hot_encoding_y(train_data)[:m]\n",
    "y = np.reshape(y, (m, 10))\n",
    "\n",
    "layers=[256, 10]\n",
    "activation_functions = ['relu'] * (len(layers) - 1) + ['softmax']\n",
    "hp = HyperParameters(layers=layers, learning_rate=.5, epochs=500, mini_batch_size=2048, lambd=.1)\n",
    "\n",
    "nn = NeuralNetwork(X.T, y.T, hp, activation_functions=activation_functions)\n",
    "\n",
    "nn.train_nn( verbose=True, per_epoch_log=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba5ac4ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T05:49:51.282847Z",
     "iopub.status.busy": "2022-02-10T05:49:51.281372Z",
     "iopub.status.idle": "2022-02-10T05:49:51.882284Z",
     "shell.execute_reply": "2022-02-10T05:49:51.883357Z",
     "shell.execute_reply.started": "2022-02-09T19:34:44.442038Z"
    },
    "papermill": {
     "duration": 0.702289,
     "end_time": "2022-02-10T05:49:51.883677",
     "exception": false,
     "start_time": "2022-02-10T05:49:51.181388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of examples: 42000\n",
      "number of right predictions: 42000\n",
      "number of wrong predictions: 0\n",
      "accuracy on train: 100.0%\n"
     ]
    }
   ],
   "source": [
    "prob_preds = lambda preds: (preds == preds.max(axis=1)[:,None]).astype(int)\n",
    "preds = nn.predict(X.T)\n",
    "r = np.sum(np.argmax(y, axis=1) == np.argmax(preds, axis=1))\n",
    "w = np.sum(np.argmax(y, axis=1) != np.argmax(preds, axis=1))\n",
    "print(f\"total number of examples: {m}\\nnumber of right predictions: {r}\\nnumber of wrong predictions: {w}\\n\"\n",
    "          f\"accuracy on train: {r/m * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eaa416e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T05:49:51.975284Z",
     "iopub.status.busy": "2022-02-10T05:49:51.974534Z",
     "iopub.status.idle": "2022-02-10T05:49:54.440317Z",
     "shell.execute_reply": "2022-02-10T05:49:54.439650Z",
     "shell.execute_reply.started": "2022-02-09T19:34:44.976884Z"
    },
    "papermill": {
     "duration": 2.504396,
     "end_time": "2022-02-10T05:49:54.440497",
     "exception": false,
     "start_time": "2022-02-10T05:49:51.936101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "462cbfdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T05:49:54.495769Z",
     "iopub.status.busy": "2022-02-10T05:49:54.495080Z",
     "iopub.status.idle": "2022-02-10T05:49:55.018727Z",
     "shell.execute_reply": "2022-02-10T05:49:55.017642Z",
     "shell.execute_reply.started": "2022-02-09T19:34:46.188094Z"
    },
    "papermill": {
     "duration": 0.553696,
     "end_time": "2022-02-10T05:49:55.018996",
     "exception": false,
     "start_time": "2022-02-10T05:49:54.465300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = test_data.to_numpy() / 255\n",
    "preds = nn.predict(X_test.T)\n",
    "preds = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c745f50a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-10T05:49:55.121426Z",
     "iopub.status.busy": "2022-02-10T05:49:55.120705Z",
     "iopub.status.idle": "2022-02-10T05:49:55.173823Z",
     "shell.execute_reply": "2022-02-10T05:49:55.173191Z",
     "shell.execute_reply.started": "2022-02-09T19:34:46.623741Z"
    },
    "papermill": {
     "duration": 0.108499,
     "end_time": "2022-02-10T05:49:55.174013",
     "exception": false,
     "start_time": "2022-02-10T05:49:55.065514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(preds, columns=['Label'])\n",
    "sub_df.index.name= 'ImageId'\n",
    "sub_df.index = sub_df.index + 1\n",
    "sub_df.reset_index().to_csv('mnsit_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 728.512082,
   "end_time": "2022-02-10T05:49:56.113754",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-10T05:37:47.601672",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
